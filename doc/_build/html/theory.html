

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>NLP Theory &mdash; pytma 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pytma Gallery" href="auto_examples/index.html" />
    <link rel="prev" title="Welcome to pytma’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> pytma
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">NLP Theory</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pre-processing-steps">Pre-processing steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#topic-modeling-lda-ctm">Topic Modeling : LDA CTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#topic-modeling-lsi">Topic Modeling : LSI</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">pytma Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pytma</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>NLP Theory</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/theory.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nlp-theory">
<h1>NLP Theory<a class="headerlink" href="#nlp-theory" title="Permalink to this headline">¶</a></h1>
<p>Natural language processing (NLP) concerns itself with the interaction between natural human languages and computing devices. NLP is a major aspect of computational linguistics, and also falls within the realms of computer science and artificial intelligence.</p>
<div class="section" id="pre-processing-steps">
<h2>Pre-processing steps<a class="headerlink" href="#pre-processing-steps" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple" start="2">
<li><p>Tokenization</p></li>
</ol>
<p>Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized.</p>
<ol class="arabic simple" start="3">
<li><p>Normalization</p></li>
</ol>
<p>Before further processing, text needs to be normalized. Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.</p>
<ol class="arabic simple" start="4">
<li><p>Stemming</p></li>
</ol>
<p>Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.</p>
<p>running -&gt; run</p>
<ol class="arabic simple" start="5">
<li><p>Lemmatization</p></li>
</ol>
<p>Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word’s lemma.For example, stemming the word “better” would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:
better -&gt; good
It should be easy to see why the implementation of a stemmer would be the less difficult feat of the two. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.  Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma</p>
<p>A good starting point is <a class="reference internal" href="#charniak1997" id="id1"><span>[Charniak1997]</span></a>.</p>
<dl class="citation">
<dt class="label" id="charniak1997"><span class="brackets"><a class="fn-backref" href="#id1">Charniak1997</a></span></dt>
<dd><p>Statistical Language Learning Charniak E Language (1997) 73(3) 588</p>
</dd>
</dl>
</div>
<div class="section" id="topic-modeling-lda-ctm">
<h2>Topic Modeling : LDA CTM<a class="headerlink" href="#topic-modeling-lda-ctm" title="Permalink to this headline">¶</a></h2>
<p>Topic modeling is about reducing a set of documents (a corpus) to a representation as a mixture of topics.  Each topic is a distribution over the set of words in the corpus. There are two main Bayesian topic models LDA (Latent Dirichlet Allocation) <a class="reference internal" href="#bleietal2005" id="id2"><span>[BleiEtAl2005]</span></a> and CTM (Correlated Topic Model) <a class="reference internal" href="#bleietal2003" id="id3"><span>[BLeiEtAl2003]</span></a>  These are Bayesian models that use a variational formulation to fit the models. A limitation of LDA is the inability to model topic correlation.  One would expect that topics are not independent. The correlated topic model allows for the topic proportions to exhibit correlations.  This is achieved via a transformation of logistic normal distribution.  There are additional complications since we loose conjugacy in the prior. LDA is more widely used but we aim to incorporate CTM in this library. We currently use the implementation of gensim for LDA and an implementation of CTM from</p>
<p><cite>https://github.com/lewer/gensim/tree/develop/gensim/models</cite></p>
<dl class="citation">
<dt class="label" id="bleietal2005"><span class="brackets"><a class="fn-backref" href="#id2">BleiEtAl2005</a></span></dt>
<dd><p>Blei, D. M., &amp; Lafferty, J. D. (2005). Correlated topic models. In Advances in Neural Information Processing Systems (pp. 147–154).</p>
</dd>
<dt class="label" id="bleietal2003"><span class="brackets"><a class="fn-backref" href="#id3">BLeiEtAl2003</a></span></dt>
<dd><p>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, 3(4–5), 993–1022</p>
</dd>
</dl>
</div>
<div class="section" id="topic-modeling-lsi">
<h2>Topic Modeling : LSI<a class="headerlink" href="#topic-modeling-lsi" title="Permalink to this headline">¶</a></h2>
<p>Before LDA and CTM there were numerical linear algebra based methods of calculating document similarity. LSI (Latent Semantic Indexing) starts with vector representations of documents  and use the SVD (singular value decomposition) to decompose the corpus matrix A into a term-concept matrix U, a singular value matrix S, and a concept-document matrix V</p>
<div class="math notranslate nohighlight">
\[\]</div>
<p>A = USV’</p>
<p>Vector representations A are usually either a word frequencies - called bag of words (BOW) - or adjusted word frequencies (TF-IDF) that account for overall corpus word counts. The TF-IDF representation aims to account for how important a word is to a document relative to its overall appearance in a corpus of documents basically adjusting for the fact that some words appear more frequently and provide less information in the topics.</p>
<p>NNMF (non-negative matrix factorization ) can also be used to find a low dimensional representation for documents.  A we can be factored to W and H, such that A= WH. Where
W  are basis vectors representing the topics and H is a coefficient matrix representing the membership weights for the topics in each document.</p>
<p>We have exposed all of these topic models in the toolbox but focus mainly on LDA and CTM in the workflows.s</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="auto_examples/index.html" class="btn btn-neutral float-right" title="pytma Gallery" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to pytma’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Bruce Campbell

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>